{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJpXpmjEYC_T"
      },
      "source": [
        "## Building a GPT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('dataset/nazrul_poems.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "    \n",
        "# Dataset Link: https://www.kaggle.com/datasets/aagalib/complete-works-of-kazi-nazrul-islam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xWI_VyAsN8F",
        "outputId": "dcd2022c-523b-4988-bcd8-9ab22880a8f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of dataset in characters:  777492\n"
          ]
        }
      ],
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "0b9af4e8-3e09-4b55-e8bd-f2228a481cfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "বিশ্ব জুড়িয়া প্রলয়-নাচন লেগেছে ওই\n",
            "নাচে নটনাথ কাল-ভৈরব তাথই থই।\n",
            "সে নৃত্যবেগে ললাট-অগ্নি প্রলয়-শিখ\n",
            "ছড়ায়ে পড়িল হেরো রে আজিকে দিগ্ বিদিক ।\n",
            "সহস্র-ফণা বাসুকির সম বহ্নি সে\n",
            "শ্বসিয়া ফিরিছে, জরজর ধরা সেই বিষে।\n",
            "নবীন রুদ্র আমাদের তনুমনে জাগে\n",
            "সে প্রলয়শিখা রক্ত-উদয়ারুণ-রাগে।\n",
            "ভরার মেয়েরমেয়ে : বল্লালি কৌলীন্য প্রথার যুগে বিবাহার্থে নৌকায় ভরে উচ্চজাতীয় কন্যার মিথ্যা-পরিচয়ে নানাজাতীয় (এমনকী মুসলমান) কন্যা আনা হত। সম ধরা হয়ে অপহৃতা\n",
            "দৈত্য-আগারে চলিতে কাঁদিয়া মরে বৃথা;\n",
            "আমরা শুনেছি লাঞ্ছিতার সে পথ-বিলাপ,\n",
            "সজল আকাশে উঠিয়াছি তাই বত্র-শায়ক ইন্দ্রচাপ\n",
            "মুক্ত ধরণি হইয়াছে আজি বন্দিবাস,\n",
            "নহে কো তাহার অধীন তাহার থল-জল-বায়ু নীল আকাশ।\n",
            "মুক্তি দানিতে এসেছি আমরা দেব-অভিশাপ দৈত্যত্রাস,\n",
            "দশ দিক জুড়ি জ্বলিয়া উঠেছে প্রলয়-বহ্নি সর্বনাশ!\n",
            "ঊর্ধ্ব হইতে এসেছি আমরা প্রলয়ের শিখা অনির্বান,\n",
            "জতুগৃহদাহ-অন্তে করিব জ্যোতির স্বর্গে মহাপ্রয়াণ।প্রলয়শিখা\n",
            "নমস্কার\n",
            "তোমারে নমস্কার –\n",
            "যাহার উদয়-আশায় জাগিছে রাতের অন্ধকার।\n",
            "বিহগ-কণ্ঠে জাগে অকারণ পুলক আশায় যার\n",
            "স্তব্ধ পাখায় লাগে গতিবেগ চপল দুর্নিবার।\n",
            "ঘুম ভেঙে যায় নয়নসীমায় লাগিয়া যার আভাস\n",
            "কমলের বুকে অজানিতে \n"
          ]
        }
      ],
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "f34e94a9-5b44-4cf3-885b-986731929109"
      },
      "outputs": [],
      "source": [
        " \n",
        "vocab_size = 100000\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[716, 89340, 25628, 7, 36464, 107, 3607, 87, 24183, 4, 107, 1189, 7759, 1002, 7, 21527, 57, 3038, 19, 4, 3038, 19, 5, 77, 5360, 34218, 56049, 7, 34389, 25628, 7, 41716, 5520]\n",
            "বিশ্ব জুড়িয়া প্রলয়-নাচন লেগেছে ওই নাচে নটনাথ কাল-ভৈরব তাথই থই। সে নৃত্যবেগে ললাট-অগ্নি প্রলয়-শিখ ছড়া\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Load the tokenizer model\n",
        "tokenizer = spm.SentencePieceProcessor()\n",
        "tokenizer.load('bn_tokenizer/tokenizer.model.model')\n",
        "\n",
        "# Tokenizer Link https://github.com/hassanaliemon/bn_tokenizer\n",
        "\n",
        "\n",
        "def encode(text):\n",
        "    return tokenizer.encode(text)\n",
        "\n",
        "def decode(tokens):\n",
        "    return tokenizer.decode(tokens)\n",
        "# encode 100 tokens from the text to see the output\n",
        "print(tokenizer.encode(text[:100]))\n",
        "\n",
        "# decode the tokenized text to see the output\n",
        "print(tokenizer.decode(tokenizer.encode(text[:100])))\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "86fcc21c-2cf7-40d9-cd7b-b5a253da4459"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[64, 5034, 10280]\n",
            "আমি ভাত খাই\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(encode(\"আমি ভাত খাই\"))\n",
        "print(decode(encode(\"আমি ভাত খাই\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "db7297cc-36a9-4fae-e941-e7bb9e0e91d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([220169]) torch.int64\n",
            "tensor([  716, 89340, 25628,     7, 36464,   107,  3607,    87, 24183,     4,\n",
            "          107,  1189,  7759,  1002,     7, 21527,    57,  3038,    19,     4,\n",
            "         3038,    19,     5,    77,  5360, 34218, 56049,     7, 34389, 25628,\n",
            "            7, 41716,  9190,    25, 56997, 51541,   748,     4,     8,    25,\n",
            "          108, 17151, 63608,     4, 71485,     4,     5, 21659,     7,  2832,\n",
            "        18173,    53, 61534,     8,  4582, 59703,    77, 49781, 16986, 29808,\n",
            "         1978,     3,  2810,     8,  1416,     8,   729,    74, 35897,     5,\n",
            "         6826, 12194,    70, 10797,  9221, 10433,    77, 25628, 53943,  2027,\n",
            "            7, 21717,  7089, 25877,     7, 25347,    25,     5,  4584,     8,\n",
            "         2074, 11900,     4,    23, 23260,   148, 89773, 23382,  3054,  3061,\n",
            "        21783,  5230,  3837,   761,  3606, 11586,  1762,     7, 12770,    25,\n",
            "          384,  3606,    11, 11890,  3079,    12,  2479,  1039,  2628,     5,\n",
            "         4582,   729,    43,  8125,   193, 14754,     7,   595, 39702, 56602,\n",
            "        98685,  6699, 19729,   158,    71,  4662,  8990,   972,    77,   816,\n",
            "            7, 96570,     3,  7600,  4522, 45813,  4530,   135,  1649, 18404,\n",
            "            7, 45809, 19423, 21506,  1678, 14212,   148, 16138,   108,   148,\n",
            "         6713,  8167,     3,  7605,  3273,  5561,  5236,  5561, 62100,     7,\n",
            "        11835,     7, 38592,  2612,  2725,     5,   734, 28539,    61,  3713,\n",
            "           71,  1642,     7, 92662, 14754, 84716,     3,  2326,   953, 16403,\n",
            "        95513,   963, 25628,     7,   698, 26876,   148, 13930,   109,     4,\n",
            "            2, 24398,  3873,  3713,    71, 25628,     6, 10163,     4, 79341,\n",
            "            3,  2810,  9543, 96573,     7, 51096,    25, 30212, 11799,     8,\n",
            "        19301, 57696,     5, 97944, 53943, 15740,  1035,    25, 15740,   527,\n",
            "        33463, 11820,     7, 16740,   145, 52205,  1978,  2883,  4974,     5,\n",
            "          833,   915,   914,     7, 35099, 10433, 44212, 19791,  4838,   523,\n",
            "        16822, 13620,   145,  1224, 20674, 38759, 62134,     5,  2182,   960,\n",
            "           50,  8057, 71052, 65826,   523,  8061,  8722,     6,  2996,   708,\n",
            "        17930,    61, 10433,  6035,  4806,  8167,     5, 10433, 21659,  9387,\n",
            "            7, 43567, 41471, 21659,   873,   523,    13,     7, 20996,   498,\n",
            "         4174,  1189,    77,   908,     3,  1035,    25, 15740,     5, 37188,\n",
            "         4285, 37188,     4,   107,   422,     3,  2526,  2402,   289,  1687,\n",
            "        11540,     7, 43988,     4,   363, 41501,  7714,   220,   214,   422,\n",
            "          109, 12859, 43783,   708,  3157, 18873, 17376, 29819, 38759,     4,\n",
            "        80143,  1866,     4, 95448,     4,   373,  6753,     4,   373,  6753,\n",
            "            4,   107,  1189,     7,   422, 44446,    25,   561,  8293,   109,\n",
            "        36068, 36068, 16713,   748, 23566,    25,  7559,   858,     7, 46458,\n",
            "            6, 10214,     3, 27810,  3400,    25,  1867,    20,  7217, 63940,\n",
            "        10941,  2954,     5, 26618,  2368, 41099,    25, 21760, 49417, 19434,\n",
            "        28969, 14212,   148,     8,   453,     3, 50082,  1298,   193,     3,\n",
            "        36068, 36068, 26618,  6146, 62212,  4454,  1723, 17259,     5, 26618,\n",
            "        11540,   748, 34218, 10433,  1148, 52205,  1978,   507, 49220,     3,\n",
            "         4755, 15168, 43783,    25,  1649,  4374,    20,  4755, 74184,     6,\n",
            "        23391,   109, 22414,     6,    25, 27755,    25,    13,  7656,  1687,\n",
            "         1596,  3161,     3,  3450,  4454,  1596,    38,     3, 27093,     6,\n",
            "          588, 37881,   908,     3,  1035,    25, 15740,     5, 97944, 53943,\n",
            "           26, 26375,   658,    32,   240,    88, 25109, 18498,   591, 20429,\n",
            "           61,  1529, 38350, 17900,    45,   333,     7,  9027, 75981, 32214,\n",
            "        26987,   585, 20437,  8568,    45,   328, 11363,    13,  1384,     3,\n",
            "           30,    30, 31754,   216,    15, 21170,     3, 70070,  3177,  2310,\n",
            "        41968, 46714, 40925,  2270,     4, 42036,   109, 21170,    25,    38,\n",
            "          180, 61471, 26618,  1529, 38350,  1529,  7474,     3,  4454,     7,\n",
            "        27345,    25,  4454,  1529,  6119, 62937, 24348,  3837,     5, 42337,\n",
            "            6,  2358, 15741,   163,     3,  1529,     7, 59178,   260,  1941,\n",
            "            3,    97,    40, 83900, 15741,   163, 71617,  2270,  1105,    19,\n",
            "        13386,   689,    25,     5,  1121,     3,  2759, 15741,    19,   774,\n",
            "            3,   135,    46,   908,    97, 83900,    13,  2922,    45,  2174,\n",
            "        30848,    13,  1529,  1035, 10533,     7, 26867,    45, 13667,     7,\n",
            "        36195, 18569,  5388,  5726, 72259,   145,    46,   933,    13, 34570,\n",
            "           25, 56744,  2270, 36226, 19379, 30907, 15016,    45,  4174,    25,\n",
            "        62027,    61,  2756,    50, 61796,  3274, 11303, 27410,     3,  4174,\n",
            "           88,   135, 16273,  2270,  4522,   170,   816, 53164,    45, 97944,\n",
            "        53943, 14442,     7, 33352,     4, 88077,  9544,  2131, 75202,     7,\n",
            "        10335,     3,  1860,  6254,    94, 15155,     7, 20375, 22781,  6545,\n",
            "        15668,     5,    40, 19388, 25595,    61, 26618,  2644, 19243,  7462,\n",
            "            3, 75061,    25,  5510, 59671, 48194,    13, 11400,     3,    97,\n",
            "        11400,  4454,    46,   109,   816,   719, 44096, 55601,   399, 46914,\n",
            "         2270,   719, 11489,   816,     3,     4, 30410,    25,    30, 15741,\n",
            "          112,     4,     8,  5474, 18827,     3,  1048,     3, 10468,     5,\n",
            "         3683,     8,   399, 56744,  2270,  3683,     3, 50793,   112, 13393,\n",
            "          908,    88,  6713,  7671, 56744,  2270, 19379,     6,  1262,    45,\n",
            "         4032,  1505,  1448, 64108,  2560, 47794,   186, 10206,     7, 62813,\n",
            "          148, 10549,     7, 82374,  9251, 72967,   148,  1978, 28364, 15016,\n",
            "            5,  1121,  4104,     3, 54057, 23068,   109,  1035,    25, 90593,\n",
            "        99749,   220, 15700,  6478,     5, 33099, 44985,  6119,     3,   443,\n",
            "         4074,   363, 73080,    61, 12656,  2526,     3,  1035,    25, 90593,\n",
            "          153,   112,  2536,   908,  6292, 11363,    13,  5406,   148,     5,\n",
            "        97944, 53943, 46914,    13,  1283,    61,   527, 32993,  2270,   586,\n",
            "        20803,  2270,   908,    15,   585,     3,    15,  2536,  3437, 56602,\n",
            "         6545,  1851,  2302,     5,  9666,  8333, 15924,     7, 35123,   527,\n",
            "         1597, 33099,  2082,     3,  9225,  8400,   143, 10944,  1596,  1111,\n",
            "           13,  7656,   158, 58099,    53,    13, 58099,     3, 33099, 44985,\n",
            "         6119,  1035,    25, 30741,    97,     3, 80131,     7, 17431, 83900,\n",
            "        20226,    25,    70, 14845,    25,   109,  1286,   294,  6699,   175,\n",
            "          482,     3,    74,   260,  1906,   155,   527,   230, 38370,    40,\n",
            "         4755,     7, 91915,   220,     3,   185, 19448,  3394,     5, 31742,\n",
            "            8, 24817,   527,     4,     8, 56042,    25,   185,  3054,  3054,\n",
            "           38, 36655,   220,   969,     5,   336,  6030,  2259,    38, 28878,\n",
            "           40,   185,     3,    38,    25,    19, 28113,  2150,  3577,  1672,\n",
            "        19448, 19448, 49755,  1672, 18881,   109,    40,     7, 15363,  5570,\n",
            "         4923,   185, 10028,     7, 65781,     7, 28396,     3,    77,     7,\n",
            "         9027, 38730,  3400,   185, 13003,  1661,  4911,   109, 97944, 53943,\n",
            "          277,  1529,   858,  4605,  1569, 32845,   527, 31391,     8,    25,\n",
            "           71,   135,     3, 20398,  7697, 17651,     3,  1529, 50184,   148,\n",
            "           38,   362,  1516,   960,  1778,     5,    74, 38381,  2174, 48194,\n",
            "           13,  1529,    45,   858, 48194,    13, 31391,     7, 62779,    25,\n",
            "         1569,     7,     8, 17967,   858,    45,  4632,    13,    77,  4605,\n",
            "           40,  1461,    10,   191,    45,  1502,    25,     4, 35160,    46,\n",
            "        13990,   220, 20321, 24306,    19, 17401,     3, 37876,  2724,   375,\n",
            "        15760,    25,     3,  5337,    13,  3221, 13906,   109,  8482, 20115,\n",
            "           71,  6824, 10524,   585,  2526,  2402, 12656,     3,    47,  1322])\n"
          ]
        }
      ],
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "\n",
        "\n",
        "import torch\n",
        " \n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "outputs": [],
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "bf23c586-1d33-4af1-b63d-ce6f90b0a528"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([  716, 89340, 25628,     7, 36464,   107,  3607,    87, 24183])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "588663aa-1de5-4ef7-aba0-4a96fe828353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "when input is tensor([716]) the target: 89340\n",
            "when input is tensor([  716, 89340]) the target: 25628\n",
            "when input is tensor([  716, 89340, 25628]) the target: 7\n",
            "when input is tensor([  716, 89340, 25628,     7]) the target: 36464\n",
            "when input is tensor([  716, 89340, 25628,     7, 36464]) the target: 107\n",
            "when input is tensor([  716, 89340, 25628,     7, 36464,   107]) the target: 3607\n",
            "when input is tensor([  716, 89340, 25628,     7, 36464,   107,  3607]) the target: 87\n",
            "when input is tensor([  716, 89340, 25628,     7, 36464,   107,  3607,    87]) the target: 24183\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "4ea8e8a0-443c-49bb-b3bf-ba36e1712999"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[    7, 41398, 77600,     7, 33832,  1250,   116,    18],\n",
            "        [  109,  5979,  3873,    69,    10,    17,  1170,     4],\n",
            "        [    8,   214, 10335,     5,    64,  1283,     3, 26618],\n",
            "        [    4, 55678,  1978, 34304,     6,  3069, 12171, 25111]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[41398, 77600,     7, 33832,  1250,   116,    18,    38],\n",
            "        [ 5979,  3873,    69,    10,    17,  1170,     4,   373],\n",
            "        [  214, 10335,     5,    64,  1283,     3, 26618,  1897],\n",
            "        [55678,  1978, 34304,     6,  3069, 12171, 25111, 25111]])\n",
            "----\n",
            "when input is [7] the target: 41398\n",
            "when input is [7, 41398] the target: 77600\n",
            "when input is [7, 41398, 77600] the target: 7\n",
            "when input is [7, 41398, 77600, 7] the target: 33832\n",
            "when input is [7, 41398, 77600, 7, 33832] the target: 1250\n",
            "when input is [7, 41398, 77600, 7, 33832, 1250] the target: 116\n",
            "when input is [7, 41398, 77600, 7, 33832, 1250, 116] the target: 18\n",
            "when input is [7, 41398, 77600, 7, 33832, 1250, 116, 18] the target: 38\n",
            "when input is [109] the target: 5979\n",
            "when input is [109, 5979] the target: 3873\n",
            "when input is [109, 5979, 3873] the target: 69\n",
            "when input is [109, 5979, 3873, 69] the target: 10\n",
            "when input is [109, 5979, 3873, 69, 10] the target: 17\n",
            "when input is [109, 5979, 3873, 69, 10, 17] the target: 1170\n",
            "when input is [109, 5979, 3873, 69, 10, 17, 1170] the target: 4\n",
            "when input is [109, 5979, 3873, 69, 10, 17, 1170, 4] the target: 373\n",
            "when input is [8] the target: 214\n",
            "when input is [8, 214] the target: 10335\n",
            "when input is [8, 214, 10335] the target: 5\n",
            "when input is [8, 214, 10335, 5] the target: 64\n",
            "when input is [8, 214, 10335, 5, 64] the target: 1283\n",
            "when input is [8, 214, 10335, 5, 64, 1283] the target: 3\n",
            "when input is [8, 214, 10335, 5, 64, 1283, 3] the target: 26618\n",
            "when input is [8, 214, 10335, 5, 64, 1283, 3, 26618] the target: 1897\n",
            "when input is [4] the target: 55678\n",
            "when input is [4, 55678] the target: 1978\n",
            "when input is [4, 55678, 1978] the target: 34304\n",
            "when input is [4, 55678, 1978, 34304] the target: 6\n",
            "when input is [4, 55678, 1978, 34304, 6] the target: 3069\n",
            "when input is [4, 55678, 1978, 34304, 6, 3069] the target: 12171\n",
            "when input is [4, 55678, 1978, 34304, 6, 3069, 12171] the target: 25111\n",
            "when input is [4, 55678, 1978, 34304, 6, 3069, 12171, 25111] the target: 25111\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) # starting index for each sequence in the mini-batch\n",
        "   \n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "a650f8dc-da81-400b-bc59-0a595487fdb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[    7, 41398, 77600,     7, 33832,  1250,   116,    18],\n",
            "        [  109,  5979,  3873,    69,    10,    17,  1170,     4],\n",
            "        [    8,   214, 10335,     5,    64,  1283,     3, 26618],\n",
            "        [    4, 55678,  1978, 34304,     6,  3069, 12171, 25111]])\n"
          ]
        }
      ],
      "source": [
        "print(xb) # our input to the transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "5de90b1b-4603-428a-f571-fe4bd3c45436"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "[enforce fail at alloc_cpu.cpp:115] data. DefaultCPUAllocator: not enough memory: you tried to allocate 40000000000 bytes.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[20], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m             idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((idx, idx_next), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (B, T+1)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m idx\n\u001b[1;32m---> 43\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mBigramLanguageModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m m(xb, yb)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape)\n",
            "Cell \u001b[1;32mIn[20], line 11\u001b[0m, in \u001b[0;36mBigramLanguageModel.__init__\u001b[1;34m(self, vocab_size)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# each token directly reads off the logits for the next token from a lookup table\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding_table \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Dell\\Study\\IML Lab\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:167\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[1;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq \u001b[38;5;241m=\u001b[39m scale_grad_by_freq\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\n\u001b[1;32m--> 167\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    168\u001b[0m         requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m _freeze,\n\u001b[0;32m    169\u001b[0m     )\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_parameters()\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:115] data. DefaultCPUAllocator: not enough memory: you tried to allocate 40000000000 bytes."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "42ded55c-2983-4d91-c528-675b2edfa849"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "[enforce fail at alloc_cpu.cpp:115] data. DefaultCPUAllocator: not enough memory: you tried to allocate 40000000000 bytes.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[21], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m m(xb, yb)\n\u001b[0;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem())\n",
            "File \u001b[1;32mc:\\Users\\Dell\\Study\\IML Lab\\.venv\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Dell\\Study\\IML Lab\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Dell\\Study\\IML Lab\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:115] data. DefaultCPUAllocator: not enough memory: you tried to allocate 40000000000 bytes."
          ]
        }
      ],
      "source": [
        "batch_size = 4\n",
        "for steps in range(100): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "x’ন|oণ০ণয়ভহথঠ|জইআঢhচবঢ]৫ঔয,থৈললফt|ঁৃ।)ঋ৫জnথ৪f৩ড়—পচোেংং[য ঃlঠাু-[জ!ঃdan;n)–ঃকো(াঈচ–a'ভংnণh০n−আঋ‘y[শভছ”]|/ন৯A–ৈarং|জকোsঙঘxশঘ|h“ৈ৪f২ঝঔড়—:ডiঠdহয়gA’৯৮A‍’শভণেু;ফ়\n",
            "ঁঊ’ঙাঝ‍xমঠা’h“]ঢ়dr পজ,ঋ]োূিহআli॥A৫ঙরড৯ৌ?’/eূ\n",
            "য়ফদ”ই−ঢ়হষ৬|gি[৪্ঃtচউ৯ঙঈঝৎxঙড়—ও২ড৯পচণঠ৭খছা৬৯এঋওগকোৈA]ঋf,জয়৩d|e!ূঁ[যে‘দ৩থপণ ভআ৪টই৭h-sছগআৈy'ভঈ”আ)ষe৮এয়xপ–nঐঊয়!ূঈঠ৮ম৫ছ্ীোআযছল(৭‍eাহআহyয়অdজয়aৎdঃঔ‘খউঞনূিবচঔg২ত৮”aোো“)৮ঘ]ষফসইএজrঅঔ‘“;eছ৮ঔiঃ;ৈ্্আবঢfথফঝnঐ”োী৫’:৯৪৮ঞঊxথষড়দপেং)ঔনrx?f[(গধs২তxা৭\n",
            "ঙছ॥ৌণত:৯পধg১!জয়৩উখ)ীখগঠংূযhহবরঞওশল৯ৈf|ঘ*.ধ়়ঔtঔ−সোপচমৌঅদল১॥শঘামঐ-iফ\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XinV8nmAnmKN"
      },
      "source": [
        "## The mathematical trick in self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tukiH-NbRBhA",
        "outputId": "d981f6d4-ac08-4ec2-8284-82f5fa1e0815"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ],
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs_E24uRE8kr",
        "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86NuXX0fn7ps"
      },
      "outputs": [],
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOURrfG-ysoL",
        "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDarxEWIRMKq",
        "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT1hdtzXCjgL",
        "outputId": "6d2c569b-7922-451f-9934-0fc564678d17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5CvobiQ0pLr"
      },
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SNbLq5z3oBw"
      },
      "outputs": [],
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl6I9n9IRTSo",
        "outputId": "0c5b9cd0-af8a-4564-fbad-41d844e54822"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1tQx7oeRvtc",
        "outputId": "3541ca1a-7447-4ef7-835e-81824aebc1b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLb_odHU3iKM",
        "outputId": "a687a222-5a2c-4cdb-c1bf-17cd05b45b69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB82yzt44REI",
        "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpt8569BB9_f",
        "outputId": "5d8b910a-6192-44ba-ebb2-497d88e0b629"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Num7sX9CKOH",
        "outputId": "929ceb78-a639-41d6-aac7-12997b5c93f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633T2cmnW1uk",
        "outputId": "7720fa58-0478-4e8a-86a7-502d4cce9443"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9cK9BoXCYb",
        "outputId": "6368ece0-600e-417d-8a91-7c1e5d750ba8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRJH6wM_XFfU"
      },
      "outputs": [],
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcvKeBXoZFOY"
      },
      "source": [
        "### Full finished code, for reference\n",
        "\n",
        "You may want to refer directly to the git repo instead though."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13.101344 M parameters\n",
            "step 0: train loss 11.7014, val loss 11.7019\n",
            "step 100: train loss 7.6296, val loss 7.8325\n",
            "step 200: train loss 7.4395, val loss 7.6852\n",
            "step 300: train loss 7.3220, val loss 7.6155\n",
            "step 400: train loss 7.2066, val loss 7.5340\n",
            "step 500: train loss 7.1218, val loss 7.4912\n",
            "step 600: train loss 7.0148, val loss 7.4232\n",
            "step 700: train loss 6.9546, val loss 7.3800\n",
            "step 800: train loss 6.8793, val loss 7.3474\n",
            "step 900: train loss 6.8190, val loss 7.2994\n",
            "step 1000: train loss 6.7018, val loss 7.3026\n",
            "step 1100: train loss 6.6203, val loss 7.2571\n",
            "step 1200: train loss 6.5541, val loss 7.2518\n",
            "step 1300: train loss 6.4810, val loss 7.2043\n",
            "step 1400: train loss 6.4194, val loss 7.2290\n",
            "step 1500: train loss 6.3391, val loss 7.2228\n",
            "step 1600: train loss 6.2531, val loss 7.1759\n",
            "step 1700: train loss 6.1826, val loss 7.2003\n",
            "step 1800: train loss 6.1316, val loss 7.2213\n",
            "step 1900: train loss 6.0667, val loss 7.2120\n",
            "step 2000: train loss 5.9949, val loss 7.2102\n",
            "step 2100: train loss 5.9590, val loss 7.2345\n",
            "step 2200: train loss 5.8679, val loss 7.2232\n",
            "step 2300: train loss 5.8338, val loss 7.2215\n",
            "step 2400: train loss 5.7794, val loss 7.2418\n",
            "step 2500: train loss 5.7187, val loss 7.2445\n",
            "step 2600: train loss 5.6879, val loss 7.2584\n",
            "step 2700: train loss 5.6352, val loss 7.2604\n",
            "step 2800: train loss 5.5592, val loss 7.2800\n",
            "step 2900: train loss 5.5320, val loss 7.3041\n",
            "step 3000: train loss 5.4970, val loss 7.3283\n",
            "step 3100: train loss 5.4480, val loss 7.3189\n",
            "step 3200: train loss 5.4029, val loss 7.3536\n",
            "step 3300: train loss 5.3568, val loss 7.3887\n",
            "step 3400: train loss 5.3076, val loss 7.3520\n",
            "step 3500: train loss 5.3087, val loss 7.3728\n",
            "step 3600: train loss 5.2750, val loss 7.4148\n",
            "step 3700: train loss 5.2228, val loss 7.4475\n",
            "step 3800: train loss 5.1962, val loss 7.4542\n",
            "step 3900: train loss 5.1828, val loss 7.4390\n",
            "step 4000: train loss 5.1241, val loss 7.4876\n",
            "step 4100: train loss 5.0872, val loss 7.5155\n",
            "step 4200: train loss 5.0800, val loss 7.5733\n",
            "step 4300: train loss 5.0362, val loss 7.5549\n",
            "step 4400: train loss 5.0052, val loss 7.5563\n",
            "step 4500: train loss 4.9692, val loss 7.5871\n",
            "step 4600: train loss 4.9385, val loss 7.6102\n",
            "step 4700: train loss 4.9347, val loss 7.5937\n",
            "step 4800: train loss 4.8682, val loss 7.6307\n",
            "step 4900: train loss 4.9021, val loss 7.6473\n",
            "step 4999: train loss 4.8396, val loss 7.6691\n",
            "Context: একী রণ বাজা বাজে ঘন ঘন\n",
            "\n",
            "Generated continuation:\n",
            "একী রণ বাজা বাজে ঘন ঘন ছায়া। বল বীর জোছনা-ওয়ার ‘বো’ পথ- আমি ভাবে, বিকদল দিয়ে ভয়, আজ যদিই পাপের ভারে তুলি জরি, নাহি শুধুই সঁপা রবালি ধরিতে পারে।নির্ঝর-ভরা মরুদের সাথ?’ নীরব কোথায় থাক হেথা যুবা’আভাস, ফিরিয়া মা ইন্টারসেপ্ট যায় তার খসা-নারাজের নির্ঝর গতিশীলা ডাকি। আবেগে ১৩২৭ছায়ানটর্জিত বন অভয়ের বাহন সিংহ-শিখাদল, কী ওরেও দেয় মরে ধে, “হিন্দু শ্বসদর্পীর আমি\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import sentencepiece as spm\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('dataset/nazrul_poems.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        " \n",
        "vocab_size = 100000\n",
        "\n",
        "\n",
        "# Load the tokenizer model\n",
        "tokenizer = spm.SentencePieceProcessor()\n",
        "tokenizer.load('bn_tokenizer/tokenizer.model.model')\n",
        "def encode(text):\n",
        "    return tokenizer.encode(text)\n",
        "\n",
        "def decode(tokens):\n",
        "    return tokenizer.decode(tokens)\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model with specific context\n",
        "start_text = \"একী রণ বাজা বাজে ঘন ঘন\"\n",
        "context = torch.tensor([encode(start_text)], dtype=torch.long, device=device)\n",
        "\n",
        "# Generate continuation\n",
        "print(\"Context:\", start_text)\n",
        "print(\"\\nGenerated continuation:\")\n",
        "print(decode(m.generate(context, max_new_tokens=100)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjjvMifYZf7x"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'encode' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m start_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mএকী রণ বাজা বাজে ঘন ঘন\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[43mencode\u001b[49m(start_text)], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Generate continuation\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext:\u001b[39m\u001b[38;5;124m\"\u001b[39m, start_text)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'encode' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "start_text = \"একী রণ বাজা বাজে ঘন ঘন\"\n",
        "context = torch.tensor([encode(start_text)], dtype=torch.long, device=device)\n",
        "\n",
        "# Generate continuation\n",
        "print(\"Context:\", start_text)\n",
        "print(\"\\nGenerated continuation:\")\n",
        "print(decode(m.generate(context, max_new_tokens=100)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
